{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction (IE) is the task of identifying named entities and semantic relations between these entities in text data. In this lab we will focus on two sub-tasks in IE, **named entity recognition** (identifying mentions of entities) and **entity linking** (matching these mentions to entities in a knowledge base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder about our [Rules for hand-in assignments](https://www.ida.liu.se/~TDDE16/exam.en.shtml#handins) and the [Policy on cheating and plagiarism](https://www.ida.liu.se/~TDDE16/exam.en.shtml#cheating)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we will be using has been tokenized following the conventions of the [Penn Treebank](ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html), and we need to prevent spaCy from using its own tokenizer on top of this. We therefore override spaCy&rsquo;s tokenizer with one that simply splits on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return Doc(self.vocab, words=text.split(' '))\n",
    "\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data set for this lab is a collection of news wire articles in which mentions of named entities have been annotated with page names from the [English Wikipedia](https://en.wikipedia.org/wiki/). The next code cell loads the training and the development parts of the data into Pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with bz2.open('ner-train.tsv.bz2', 'rt', encoding='utf-8') as source:\n",
    "    df_train = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with bz2.open('ner-dev.tsv.bz2', 'rt', encoding='utf-8') as source:\n",
    "    df_dev = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in these two data frames corresponds to one mention of a named entity and has five columns:\n",
    "\n",
    "1. a unique identifier for the sentence containing the entity mention\n",
    "2. the pre-tokenized sentence, with tokens separated by spaces\n",
    "3. the start position of the token span containing the entity mention\n",
    "4. the end position of the token span (exclusive, as in Python list indexing)\n",
    "5. the entity label; either a Wikipedia page name or the generic label `--NME--`\n",
    "\n",
    "The following cell prints the first five samples from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000-001</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000-002</td>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brussels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                          sentence  beg  end  \\\n",
       "0    0000-000  EU rejects German call to boycott British lamb .    0    1   \n",
       "1    0000-000  EU rejects German call to boycott British lamb .    2    3   \n",
       "2    0000-000  EU rejects German call to boycott British lamb .    6    7   \n",
       "3    0000-001                                   Peter Blackburn    0    2   \n",
       "4    0000-002                               BRUSSELS 1996-08-22    0    1   \n",
       "\n",
       "            label  \n",
       "0         --NME--  \n",
       "1         Germany  \n",
       "2  United_Kingdom  \n",
       "3         --NME--  \n",
       "4        Brussels  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we see that the first sentence is annotated with three entity mentions:\n",
    "\n",
    "* the span 0–1 &lsquo;EU&rsquo; is annotated as a mention but only labelled with the generic `--NME--`\n",
    "* the span 2–3 &lsquo;German&rsquo; is annotated with the page [Germany](http://en.wikipedia.org/wiki/Germany)\n",
    "* the span 6–7 &lsquo;British&rsquo; is annotated with the page [United_Kingdom](http://en.wikipedia.org/wiki/United_Kingdom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To warm up, we ask you to write code to print the three measures that you will be using for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    \"\"\"Print precision, recall, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        Nothing, but prints the precision, recall, and F1 values computed\n",
    "        based on the specified sets.\n",
    "    \"\"\"\n",
    "    # TODO: Replace the next line with your own code\n",
    "    prec = len(gold.intersection(pred))/len(pred) *100\n",
    "    print('The precision is {}%'.format('%.2f'%prec))\n",
    "    reca = len(gold.intersection(pred))/len(gold) *100\n",
    "    print('The recall is {}%'.format('%.2f'%reca))\n",
    "    f1 = 2*(prec*reca)/(prec+reca)\n",
    "    print('The F1-value is {}%'.format('%.2f'%f1))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 60.00%\n",
      "The recall is 100.00%\n",
      "The F1-value is 75.00%\n"
     ]
    }
   ],
   "source": [
    "evaluation_report(set(range(3)), set(range(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a precision of 60%, a recall of 100%, and an F1-value of 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first tasks that an information extraction system has to solve is to locate and classify (mentions of) named entities, such as persons and organizations. Here we will tackle the simpler task of recognizing **spans** of tokens that contain an entity mention, without the actual entity label.\n",
    "\n",
    "The English language model in spaCy features a full-fledged [named entity recognizer](https://spacy.io/usage/linguistic-features#named-entities) that identifies a variety of entities, and can be updated with new entity types by the user. Your task in this problem is to evaluate the performance of this component when predicting entity spans in the development data.\n",
    "\n",
    "Start by implementing a generator function that yields the gold-standard spans in a given data frame.\n",
    "\n",
    "**Hint:** The Pandas method [`itertuples()`](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.itertuples.html) is useful when iterating over the rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_spans(df):\n",
    "    \"\"\"Yield the gold-standard mention spans in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "    # TODO: Replace the next line with your own code\n",
    "    for row in df.iloc[:,[0,2,3]].itertuples(index=False, name=None):\n",
    "        yield row\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can count the spans yielded by your function. When called on the development data, you should get a total of 5,917 unique triples. The first triple and the last triple should be\n",
    "\n",
    "    ('0946-000', 2, 3)\n",
    "    ('1161-010', 1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5917\n",
      "('0946-000', 2, 3)\n",
      "('1161-010', 1, 3)\n"
     ]
    }
   ],
   "source": [
    "spans_dev_gold = set(gold_spans(df_dev))\n",
    "print(len(spans_dev_gold))\n",
    "spans_dev_gold_p2 = list(gold_spans(df_dev))\n",
    "print(spans_dev_gold_p2[0])\n",
    "print(spans_dev_gold_p2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to write code that calls spaCy to predict the named entities in the development data, and to evaluate the accuracy of these predictions in terms of precision, recall, and F1. Print these scores using the function that you wrote for Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to run and evaluate the spaCy NER on the development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pred_entity(df):\n",
    "  for row in df.iloc[:, [0,1]].drop_duplicates().itertuples(index=False): # .drop_duplicates()\n",
    "    for ent in nlp(row[1]).ents:\n",
    "      yield (row[0], ent.start, ent.end)\n",
    "pred_p2 = Pred_entity(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_p2 = gold_spans(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 53.43%\n",
      "The recall is 69.27%\n",
      "The F1-value is 60.33%\n"
     ]
    }
   ],
   "source": [
    "evaluation_report(set(gold_p2), set(pred_p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you were able to see in Problem&nbsp;2, the span accuracy of the named entity recognizer is far from perfect. In particular, only slightly more than half of the predicted spans are correct according to the gold standard. Your next task is to analyse this result in more detail.\n",
    "\n",
    "Here is a function that prints the false positives as well as the false negatives spans for a data frame, given a reference set of gold-standard spans and a candidate set of predicted spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def error_report(df, spans_gold, spans_pred):\n",
    "    false_pos = defaultdict(list)\n",
    "    for s, b, e in spans_pred - spans_gold:\n",
    "        false_pos[s].append((b, e))\n",
    "    false_neg = defaultdict(list)\n",
    "    for s, b, e in spans_gold - spans_pred:\n",
    "        false_neg[s].append((b, e))\n",
    "    for row in df.drop_duplicates('sentence_id').itertuples():\n",
    "        if row.sentence_id in false_pos or row.sentence_id in false_neg:\n",
    "            print('Sentence:', row.sentence)\n",
    "            for b, e in false_pos[row.sentence_id]:\n",
    "                print('  FP:', ' '.join(row.sentence.split()[b:e]))\n",
    "            for b, e in false_neg[row.sentence_id]:\n",
    "                print('  FN:', ' '.join(row.sentence.split()[b:e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to inspect and analyse the errors that the automated prediction makes. Can you see any patterns? Base your analysis on the first 500 rows of the training data. Summarize your observations in a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to do your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "  FN: LEICESTERSHIRE\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "  FP: Friday\n",
      "  FP: 39\n",
      "  FP: two days\n",
      "  FP: 38\n",
      "  FP: four\n",
      "  FN: Somerset\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "  FN: Nottinghamshire\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "  FP: 83\n",
      "  FP: first\n",
      "  FP: 94\n",
      "  FP: the opening morning\n",
      "  FP: three\n",
      "  FP: 83\n",
      "  FP: 296\n",
      "  FN: Leicestershire\n",
      "Sentence: Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .\n",
      "  FP: 174\n",
      "  FP: 213\n",
      "  FP: second\n",
      "Sentence: Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .\n",
      "  FN: Essex\n",
      "Sentence: Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .\n",
      "  FP: one-day\n",
      "  FP: first championship century of the season\n",
      "  FP: 82\n",
      "  FP: first\n",
      "  FP: 158\n",
      "  FP: 372\n",
      "Sentence: By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging on 119 for five and praying for rain .\n",
      "  FP: 37-run\n",
      "  FP: 24\n",
      "  FP: 48\n",
      "  FP: 119\n",
      "  FP: five\n",
      "  FP: four\n",
      "  FN: Such\n",
      "Sentence: At the Oval , Surrey captain Chris Lewis , another man dumped by England , continued to silence his critics as he followed his four for 45 on Thursday with 80 not out on Friday in the match against Warwickshire .\n",
      "  FP: Friday\n",
      "  FP: four\n",
      "  FP: Thursday\n",
      "  FP: 80\n",
      "  FN: Warwickshire\n",
      "  FN: Oval\n",
      "Sentence: He was well backed by England hopeful Mark Butcher who made 70 as Surrey closed on 429 for seven , a lead of 234 .\n",
      "  FP: seven\n",
      "  FP: 234\n",
      "  FP: 429\n",
      "  FP: 70\n",
      "Sentence: Derbyshire kept up the hunt for their first championship title since 1936 by reducing Worcestershire to 133 for five in their second innings , still 100 runs away from avoiding an innings defeat .\n",
      "  FP: five\n",
      "  FP: 1936\n",
      "  FP: second\n",
      "  FP: 100\n",
      "  FP: 133\n",
      "  FP: first\n",
      "  FN: Derbyshire\n",
      "Sentence: Australian Tom Moody took six for 82 but Chris Adams , 123 , and Tim O'Gorman , 109 , took Derbyshire to 471 and a first innings lead of 233 .\n",
      "  FP: 233\n",
      "  FP: 82\n",
      "  FP: first\n",
      "  FP: 471\n",
      "  FP: 123\n",
      "  FP: 109\n",
      "  FP: six\n",
      "Sentence: After the frustration of seeing the opening day of their match badly affected by the weather , Kent stepped up a gear to dismiss Nottinghamshire for 214 .\n",
      "  FP: the opening day\n",
      "  FP: 214\n",
      "Sentence: They were held up by a gritty 84 from Paul Johnson but ex-England fast bowler Martin McCague took four for 55 .\n",
      "  FP: four\n",
      "  FP: 84\n",
      "  FP: 55\n",
      "Sentence: By stumps Kent had reached 108 for three .\n",
      "  FP: 108\n",
      "  FP: three\n",
      "Sentence: CRICKET - ENGLISH COUNTY CHAMPIONSHIP SCORES .\n",
      "  FN: ENGLISH COUNTY CHAMPIONSHIP\n",
      "Sentence: Result and close of play scores in English county championship matches on Friday :\n",
      "  FP: Friday\n",
      "Sentence: Leicester : Leicestershire beat Somerset by an innings and 39 runs .\n",
      "  FP: 39\n",
      "  FN: Leicestershire\n",
      "  FN: Leicester\n",
      "  FN: Somerset\n",
      "Sentence: Somerset 83 and 174 ( P. Simmons 4-38 ) , Leicestershire 296 .\n",
      "  FP: 174\n",
      "  FP: 296\n",
      "  FN: Leicestershire\n",
      "  FN: P. Simmons\n",
      "Sentence: Leicestershire 22 points , Somerset 4 .\n",
      "  FP: 22\n",
      "  FN: Leicestershire\n",
      "Sentence: Chester-le-Street : Glamorgan 259 and 207 ( A. Dale 69 , H. Morris 69 ; D. Blenkiron 4-43 ) , Durham 114 ( S. Watkin 4-28 ) and 81-3 .\n",
      "  FP: Durham 114\n",
      "  FP: 207\n",
      "  FP: 259\n",
      "  FP: 81-3\n",
      "  FN: Durham\n",
      "  FN: Glamorgan\n",
      "  FN: Chester-le-Street\n",
      "  FN: A. Dale\n",
      "  FN: S. Watkin\n",
      "Sentence: Tunbridge Wells : Nottinghamshire 214 ( P. Johnson 84 ; M. McCague 4-55 ) , Kent 108-3 .\n",
      "  FP: P. Johnson 84\n",
      "  FP: M. McCague 4-55\n",
      "  FP: 214\n",
      "  FN: Nottinghamshire\n",
      "  FN: M. McCague\n",
      "  FN: Kent\n",
      "  FN: Tunbridge Wells\n",
      "  FN: P. Johnson\n",
      "Sentence: London ( The Oval ) : Warwickshire 195 , Surrey 429-7 ( C. Lewis 80 not out , M. Butcher 70 , G. Kersey 63 , J. Ratcliffe 63 , D. Bicknell 55 ) .\n",
      "  FP: 55\n",
      "  FP: J. Ratcliffe 63\n",
      "  FP: 195\n",
      "  FN: Warwickshire\n",
      "  FN: J. Ratcliffe\n",
      "  FN: Surrey\n",
      "  FN: C. Lewis\n",
      "Sentence: Hove : Sussex 363 ( W. Athey 111 , V. Drakes 52 ; I. Austin 4-37 ) , Lancashire 197-8 ( W. Hegg 54 )\n",
      "  FP: 52\n",
      "  FP: I. Austin 4-37\n",
      "  FP: 111\n",
      "  FP: 363\n",
      "  FN: Hove\n",
      "  FN: Sussex\n",
      "  FN: Lancashire\n",
      "  FN: I. Austin\n",
      "Sentence: Portsmouth : Middlesex 199 and 426 ( J. Pooley 111 , M. Ramprakash 108 , M. Gatting 83 ) , Hampshire 232 and 109-5 .\n",
      "  FP: 199\n",
      "  FP: 426\n",
      "  FP: 232\n",
      "  FP: 108\n",
      "  FP: J. Pooley 111\n",
      "  FN: J. Pooley\n",
      "  FN: Hampshire\n",
      "  FN: Portsmouth\n",
      "  FN: Middlesex\n",
      "Sentence: Chesterfield : Worcestershire 238 and 133-5 , Derbyshire 471 ( J. Adams 123 , T.O'Gorman 109 not out , K. Barnett 87 ; T. Moody 6-82 )\n",
      "  FP: 238\n",
      "  FP: T. Moody 6-82\n",
      "  FP: K. Barnett 87\n",
      "  FP: T.O'Gorman 109\n",
      "  FP: J. Adams 123\n",
      "  FN: Derbyshire\n",
      "  FN: K. Barnett\n",
      "  FN: T.O'Gorman\n",
      "  FN: Worcestershire\n",
      "  FN: J. Adams\n",
      "  FN: T. Moody\n",
      "Sentence: Bristol : Gloucestershire 183 and 185-6 ( J. Russell 56 not out ) , Northamptonshire 190 ( K. Curran 52 ; A. Smith 5-68 ) .\n",
      "  FP: Northamptonshire 190\n",
      "  FP: J. Russell 56\n",
      "  FP: 183\n",
      "  FP: A. Smith 5-68\n",
      "  FN: Gloucestershire\n",
      "  FN: A. Smith\n",
      "  FN: J. Russell\n",
      "  FN: Northamptonshire\n",
      "  FN: K. Curran\n",
      "  FN: Bristol\n",
      "Sentence: CRICKET - 1997 ASHES INTINERARY .\n",
      "  FP: 1997\n",
      "  FN: ASHES\n",
      "Sentence: Australia will defend the Ashes in\n",
      "  FN: Ashes\n",
      "Sentence: starting on May 13 next year , the Test and County Cricket Board\n",
      "  FP: May 13 next year\n",
      "  FP: County Cricket Board\n",
      "  FP: Test\n",
      "  FN: Test and County Cricket Board\n",
      "Sentence: Australia will also play three one-day internationals and\n",
      "  FP: three\n",
      "Sentence: English county sides and another against British Universities ,\n",
      "  FP: British\n",
      "  FN: British Universities\n",
      "Sentence: as well as one-day matches against the Minor Counties and\n",
      "  FN: Minor Counties\n",
      "Sentence: May 13 Arrive in London\n",
      "  FP: May 13\n",
      "Sentence: May 14 Practice at Lord 's\n",
      "  FP: May 14 Practice\n",
      "  FN: Lord 's\n",
      "Sentence: May 15 v Duke of Norfolk 's XI ( at Arundel )\n",
      "  FP: May 15\n",
      "  FN: Duke of Norfolk 's XI\n",
      "  FN: Arundel\n",
      "Sentence: May 17 v Northampton\n",
      "  FP: May 17\n",
      "Sentence: May 18 v Worcestershire\n",
      "  FP: May 18\n",
      "Sentence: May 20 v Durham\n",
      "  FP: May 20\n",
      "  FN: Durham\n",
      "Sentence: May 22 First one-day international ( at Headingley ,\n",
      "  FP: May 22 First\n",
      "Sentence: Leeds )\n",
      "  FN: Leeds\n",
      "Sentence: May 24 Second one-day international ( at The Oval ,\n",
      "  FP: May 24 Second\n",
      "  FP: one-day\n",
      "  FN: The Oval\n",
      "Sentence: May 25 Third one-day international ( at Lord 's , London )\n",
      "  FP: May 25 Third\n",
      "  FP: one-day\n",
      "  FN: Lord 's\n",
      "Sentence: May 27-29 v Gloucestershire or Sussex or Surrey ( three\n",
      "  FP: three\n",
      "  FP: May 27-29\n",
      "  FN: Sussex\n",
      "  FN: Gloucestershire\n",
      "Sentence: May 31 - June 2 v Derbyshire ( three days )\n",
      "  FP: three days\n",
      "  FP: May 31 - June 2\n",
      "  FN: Derbyshire\n",
      "Sentence: June 5-9 First test match ( at Edgbaston , Birmingham )\n",
      "  FP: First\n",
      "  FP: June 5-9\n",
      "Sentence: June 14-16 v Leicestershire ( three days )\n",
      "  FP: June 14-16\n",
      "  FP: three days\n",
      "  FN: Leicestershire\n",
      "Sentence: June 19-23 Second test ( at Lord 's )\n",
      "  FP: Second\n",
      "  FP: June 19-23\n",
      "  FN: Lord 's\n",
      "Sentence: June 25-27 v British Universities ( at Oxford , three days )\n",
      "  FP: June 25-27\n",
      "  FP: British\n",
      "  FP: three days\n",
      "  FN: British Universities\n",
      "Sentence: June 28-30 v Hampshire ( three days )\n",
      "  FP: June 28-30\n",
      "  FP: three days\n",
      "  FN: Hampshire\n",
      "Sentence: July 3-7 Third test ( at Old Trafford , Manchester )\n",
      "  FP: July 3-7 Third\n",
      "  FP: Trafford\n",
      "  FN: Old Trafford\n",
      "Sentence: July 9 v Minor Counties XI\n",
      "  FP: July 9\n",
      "  FN: Minor Counties XI\n",
      "Sentence: July 12 v Scotland\n",
      "  FP: July 12\n",
      "Sentence: July 16-18 v Glamorgan ( three days )\n",
      "  FP: three days\n",
      "  FP: July 16-18\n",
      "Sentence: July 19-21 v Middlesex ( three days )\n",
      "  FP: July 19-21\n",
      "  FP: three days\n",
      "  FN: Middlesex\n",
      "Sentence: July 24-28 Fourth test ( at Headingley )\n",
      "  FP: 24-28 Fourth\n",
      "  FP: July\n",
      "Sentence: August 1-4 v Somerset ( four days )\n",
      "  FP: August 1-4\n",
      "  FP: four days\n",
      "Sentence: August 7-11 Fifth test ( at Trent Bridge , Nottingham )\n",
      "  FP: August 7-11\n",
      "  FP: Fifth\n",
      "Sentence: August 16-18 v Kent ( three days )\n",
      "  FP: August 16-18\n",
      "  FP: three days\n",
      "Sentence: August 21-25 Sixth test ( at The Oval , London ) .\n",
      "  FP: August\n",
      "  FP: Sixth\n",
      "  FN: The Oval\n",
      "Sentence: SOCCER - SHEARER NAMED AS ENGLAND CAPTAIN .\n",
      "  FP: ENGLAND CAPTAIN\n",
      "  FN: SHEARER\n",
      "  FN: ENGLAND\n",
      "Sentence: The world 's costliest footballer Alan Shearer was named as the new England captain on Friday .\n",
      "  FP: Friday\n",
      "Sentence: The 26-year-old , who joined Newcastle for 15 million pounds sterling ( $ 23.4 million ) , takes over from Tony Adams , who led the side during the European championship in June , and former captain David Platt .\n",
      "  FP: 15 million pounds\n",
      "  FP: June\n",
      "  FP: $ 23.4 million\n",
      "Sentence: Adams and Platt are both injured and will miss England 's opening World Cup qualifier against Moldova on Sunday .\n",
      "  FP: Sunday\n",
      "  FN: Adams\n",
      "Sentence: Shearer takes the captaincy on a trial basis , but new coach Glenn Hoddle said he saw no reason why the former Blackburn and Southampton skipper should not make the post his own .\n",
      "  FN: Shearer\n",
      "Sentence: \" I 'm sure there wo n't be a problem , I 'm sure Alan is the man for the job , \" Hoddle said .\n",
      "  FN: Hoddle\n",
      "Sentence: \" There were three or four people who could have done it but when I spoke to Alan he was up for it and really wanted it .\n",
      "  FP: four\n",
      "  FP: three\n",
      "Sentence: Shearer 's Euro 96 striking partner Teddy Sheringham withdrew from the squad with an injury on Friday .\n",
      "  FP: Friday\n",
      "Sentence: BELGRADE 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "  FN: BELGRADE\n",
      "Sentence: Red Star ( Yugoslavia ) beat Dinamo ( Russia ) 92-90 ( halftime\n",
      "  FP: 92-90\n",
      "Sentence: SOCCER - ROMANIA BEAT LITHUANIA IN UNDER-21 MATCH .\n",
      "  FP: SOCCER - ROMANIA BEAT LITHUANIA\n",
      "  FN: LITHUANIA\n",
      "  FN: ROMANIA\n",
      "Sentence: BUCHAREST 1996-08-30\n",
      "  FP: BUCHAREST 1996-08-30\n",
      "  FN: BUCHAREST\n",
      "Sentence: Romania beat Lithuania 2-1 ( halftime 1-1 ) in their European under-21 soccer match on Friday .\n",
      "  FP: Friday\n",
      "Sentence: Romania - Cosmin Contra ( 31st ) , Mihai Tararache ( 75th )\n",
      "  FP: Romania - Cosmin Contra (\n",
      "  FP: 75th\n",
      "  FP: 31st\n",
      "  FN: Romania\n",
      "  FN: Cosmin Contra\n",
      "Sentence: Lithuania - Danius Gleveckas ( 13rd )\n",
      "  FP: 13rd\n",
      "  FP: Lithuania - Danius Gleveckas\n",
      "  FN: Lithuania\n",
      "  FN: Danius Gleveckas\n",
      "Sentence: SOCCER - ROTOR FANS LOCKED OUT AFTER VOLGOGRAD VIOLENCE .\n",
      "  FN: VOLGOGRAD\n",
      "  FN: ROTOR\n",
      "Sentence: MOSCOW 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Rotor Volgograd must play their next home game behind closed doors after fans hurled bottles and stones at Dynamo Moscow players during a 1-0 home defeat on Saturday that ended Rotor 's brief spell as league leaders .\n",
      "  FP: Saturday\n",
      "Sentence: The head of the Russian league 's disciplinary committee , Anatoly Gorokhovsky , said on Friday that Rotor would play Lada Togliatti to empty stands on September 3 .\n",
      "  FP: September 3\n",
      "  FP: Friday\n",
      "  FP: Russian league 's\n",
      "  FN: Russian\n",
      "Sentence: The club , who put Manchester United out of last year 's UEFA Cup , were fined $ 1,000 .\n",
      "  FP: 1,000\n",
      "  FP: last year 's\n",
      "Sentence: Despite the defeat , Rotor are well placed with 11 games to play in the championship .\n",
      "  FP: 11\n",
      "Sentence: Lying three points behind Alania and two behind Dynamo Moscow , the Volgograd side have a game in hand over the leaders and two over the Moscow club .\n",
      "  FP: two\n",
      "  FP: two\n",
      "  FP: three\n",
      "Sentence: BOXING - PANAMA 'S ROBERTO DURAN FIGHTS THE SANDS OF TIME .\n",
      "  FP: BOXING - PANAMA 'S\n",
      "  FN: PANAMA\n",
      "  FN: ROBERTO DURAN\n",
      "Sentence: PANAMA CITY 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Panamanian boxing legend Roberto \" Hands of Stone \" Duran climbs into the ring on Saturday in another age-defying attempt to sustain his long career .\n",
      "  FP: Saturday\n",
      "  FP: Roberto \" Hands of Stone \"\n",
      "  FP: Duran\n",
      "  FN: Roberto \" Hands of Stone \" Duran\n",
      "Sentence: Duran , 45 , takes on little-known Mexican Ariel Cruz , 30 , in a super middleweight non-title bout in Panama City .\n",
      "  FP: Mexican Ariel Cruz\n",
      "  FP: 30\n",
      "  FP: 45\n",
      "  FN: Mexican\n",
      "  FN: Ariel Cruz\n",
      "Sentence: The fight , Duran 's first on home soil for 10 years , is being billed here as the \" Return of the Legend \" and Duran still talks as if he was in his prime .\n",
      "  FP: 10 years\n",
      "  FP: first\n",
      "  FP: Legend\n",
      "  FN: Return of the Legend\n",
      "Sentence: If he loses Saturday , it could devalue his position as one of the world 's great boxers , \" Panamanian Boxing Association President Ramon Manzanares said .\n",
      "  FP: Panamanian Boxing Association\n",
      "  FP: Saturday\n",
      "  FN: Boxing Association\n",
      "  FN: Panamanian\n",
      "Sentence: Duran , whose 97-12 record spans three decades , hopes a win in the 10-round bout will earn him a rematch against Puerto Rico 's Hector \" Macho \" Camacho .\n",
      "  FP: Puerto Rico 's\n",
      "  FP: three decades\n",
      "  FP: 97-12\n",
      "  FN: Puerto Rico\n",
      "  FN: Hector \" Macho \" Camacho\n",
      "Sentence: Camacho took a controversial points decision against the Panamanian in Atlantic City in June in a title fight .\n",
      "  FP: June\n",
      "Sentence: SQUASH - HONG KONG OPEN QUARTER-FINAL RESULTS .\n",
      "  FN: HONG KONG OPEN\n",
      "Sentence: HONG KONG 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Quarter-final results in the Hong Kong Open on Friday ( prefix number denotes seeding ) : 1 - Jansher Khan ( Pakistan ) beat Mark Cairns ( England ) 15-10 15-6 15-7\n",
      "  FP: Friday\n",
      "  FP: the Hong Kong\n",
      "  FP: 15-7\n",
      "  FP: 15-10\n",
      "  FN: Jansher Khan\n",
      "  FN: Hong Kong Open\n",
      "Sentence: 4 - Peter Nicol ( Scotland ) beat 7 - Chris Walker ( England ) 15-8 15-13 13-15 15-9\n",
      "  FP: 15-13\n",
      "  FN: Peter Nicol\n",
      "  FN: England\n",
      "  FN: Chris Walker\n",
      "Sentence: 2 - Rodney Eyles ( Australia ) beat Derek Ryan ( Ireland ) 15-6 15-9 11-15 15-10 .\n",
      "  FN: Rodney Eyles\n",
      "Sentence: SEOUL 1996-08-30\n",
      "  FP: SEOUL 1996-08-30\n",
      "  FN: SEOUL\n",
      "Sentence: Pohang 3 Ulsan 2 ( halftime 1-0 )\n",
      "  FP: Pohang 3\n",
      "  FN: Pohang\n",
      "  FN: Ulsan\n",
      "Sentence: Puchon 2 Chonbuk 1 ( halftime 1-1 )\n",
      "  FP: 2\n",
      "  FN: Puchon\n",
      "  FN: Chonbuk\n",
      "Sentence: Puchon 3 1 0 6 1 10\n",
      "  FP: 3 1\n",
      "  FN: Puchon\n",
      "Sentence: Chonan 3 0 1 13 10 9\n",
      "  FP: 3\n",
      "  FN: Chonan\n",
      "Sentence: Pohang 2 1 1 11 10 7\n",
      "  FP: Pohang 2 1 1 11 10 7\n",
      "  FN: Pohang\n",
      "Sentence: Suwan 1 3 0 7 3 6\n",
      "  FP: Suwan 1 3 0 7 3 6\n",
      "  FN: Suwan\n",
      "Sentence: Ulsan 1 0 2 8 9 3\n",
      "  FP: 1\n",
      "Sentence: Anyang 0 3 1 6 9 3\n",
      "  FN: Anyang\n",
      "Sentence: Chonnam 0 2 1 4 5 2\n",
      "  FN: Chonnam\n",
      "Sentence: Pusan 0 2 1 3 7 2\n",
      "  FN: Pusan\n",
      "Sentence: Chonbuk 0 0 3 3 7 0\n",
      "  FN: Chonbuk\n",
      "Sentence: BASEBALL - RESULTS OF S. KOREAN PROFESSIONAL GAMES .\n",
      "  FN: S. KOREAN\n",
      "Sentence: SEOUL 1996-08-30\n",
      "  FP: SEOUL 1996-08-30\n",
      "  FN: SEOUL\n",
      "Sentence: LG 2 OB 0\n",
      "  FP: LG 2 OB 0\n",
      "  FN: OB\n",
      "  FN: LG\n",
      "Sentence: Lotte 6 Hyundai 2\n",
      "  FP: 2\n",
      "  FP: 6\n",
      "Sentence: Hyundai 6 Lotte 5\n",
      "  FP: 6\n",
      "Sentence: Haitai 2 Samsung 0\n",
      "  FP: 2\n",
      "  FN: Haitai\n",
      "Sentence: Samsung 10 Haitai 3\n",
      "  FP: 10 Haitai 3\n",
      "  FN: Haitai\n",
      "Sentence: Hanwha 6 Ssangbangwool 5\n",
      "  FP: 6\n",
      "  FN: Ssangbangwool\n",
      "Sentence: Note - Lotte and Hyundai , Haitai and Samsung played two games .\n",
      "  FP: two\n",
      "  FN: Lotte\n",
      "Sentence: Haitai 64 2 43 .596 -\n",
      "  FN: Haitai\n",
      "Sentence: Ssangbangwool 59 2 49 .545 5 1/2\n",
      "  FP: 5 1/2\n",
      "  FN: Ssangbangwool\n",
      "Sentence: Hanwha 58 1 49 .542 6\n",
      "  FP: 6\n",
      "  FP: 58\n",
      "Sentence: Hyundai 57 5 49 .536 6 1/2\n",
      "  FP: 57 5 49 .536 6 1/2\n",
      "Sentence: Samsung 49 5 56 .468 14\n",
      "  FP: 49 5\n",
      "  FP: 14\n",
      "Sentence: Lotte 46 6 54 .462 14 1/2\n",
      "  FP: 46 6 54\n",
      "  FP: 14 1/2\n",
      "Sentence: LG 46 5 59 .441 17\n",
      "  FP: 46 5 59\n",
      "  FP: 17\n",
      "  FN: LG\n",
      "Sentence: OB 42 6 62 .409 20 1/2\n",
      "  FP: 20 1/2\n",
      "Sentence: TENNIS - FRIDAY 'S RESULTS FROM THE U.S. OPEN .\n",
      "  FP: U.S.\n",
      "  FN: U.S. OPEN\n",
      "Sentence: Results from the U.S. Open Tennis Championships at the National Tennis Centre on Friday ( prefix number denotes seeding ) :\n",
      "  FP: Friday\n",
      "  FP: the National Tennis Centre\n",
      "  FN: National Tennis Centre\n",
      "  FN: U.S. Open Tennis Championships\n",
      "Sentence: Sandrine Testud ( France ) beat Ines Gorrochategui ( Argentina ) 4-6 6-2 6-1\n",
      "  FP: 6-1\n",
      "Sentence: 4 - Goran Ivanisevic ( Croatia ) beat Scott Draper ( Australia ) 6-7 ( 1-7 ) 6-3 6-4 6-4\n",
      "  FP: 4\n",
      "  FP: 1-7\n",
      "  FN: Australia\n",
      "Sentence: Mark Philippoussis ( Australia ) beat Andrei Olhovskiy ( Russia ) 6 - 3 6-4 6-2\n",
      "  FP: 6 - 3\n",
      "Sentence: Sjeng Schalken ( Netherlands ) beat David Rikl ( Czech Republic ) 6 - 2 6-4 6-4\n",
      "  FP: 6 - 2\n",
      "Sentence: Guy Forget ( France ) beat 17 - Felix Mantilla ( Spain ) 6-4 7-5 6-3\n",
      "  FP: 17\n",
      "  FN: Felix Mantilla\n",
      "Sentence: Alexander Volkov ( Russia ) beat Mikael Tillstrom ( Sweden ) 1-6 6- 4 6-1 4-6 7-6 ( 10-8 )\n",
      "  FP: 4\n",
      "  FP: 10-8\n",
      "Sentence: Jonas Bjorkman ( Sweden ) beat David Nainkin ( South Africa ) ) 6-4 6-1 6-1\n",
      "  FP: 6-4 6-1\n",
      "Sentence: 8 - Lindsay Davenport ( U.S. ) beat Anne-Gaelle Sidot ( France ) 6-0 6-3\n",
      "  FP: Davenport\n",
      "  FP: Sidot\n",
      "  FN: Lindsay Davenport\n",
      "  FN: Anne-Gaelle Sidot\n",
      "Sentence: 4 - Conchita Martinez ( Spain ) beat Helena Sukova ( Czech Republic ) 6-4 6-3\n",
      "  FN: Conchita Martinez\n",
      "Sentence: Add Men 's singles , second round 16 - Cedric Pioline ( France ) beat Roberto Carretero ( Spain ) 4-6 6 - 2 6-2 6-1 Alex Corretja ( Spain ) beat Filippo Veglio ( Switzerland ) 6-7 ( 4- 7 ) 6-4 6-4 6-0\n",
      "  FP: 4-6\n",
      "  FP: 7\n",
      "  FP: second\n",
      "  FP: 16\n",
      "  FN: Cedric Pioline\n",
      "Sentence: Add Women 's singles , third round Linda Wild ( U.S. ) beat Barbara Rittner ( Germany ) 6-4 4-6 7-5 Asa Carlsson ( Sweden ) beat 15 - Gabriela Sabatini ( Argentina ) 7-5 3-6 6-2\n",
      "  FP: 15\n",
      "  FP: 6-4 4-6 7-5 Asa Carlsson\n",
      "  FP: third\n",
      "  FN: Asa Carlsson\n",
      "  FN: Gabriela Sabatini\n",
      "Sentence: Add Men 's singles , second round 1 - Pete Sampras ( U.S. ) beat Jiri Novak ( Czech Republic ) 6-3 1-6 6-3 4-6 6-4 Paul Haarhuis ( Netherlands ) beat Michael Tebbutt ( Australia ) 1- 6 6-2 6-2 6-3\n",
      "  FP: 6-2\n",
      "  FP: second\n",
      "  FP: 6\n",
      "  FN: Pete Sampras\n",
      "  FN: Paul Haarhuis\n",
      "Sentence: Add Women 's singles , third round Lisa Raymond ( U.S. ) beat Kimberly Po ( U.S. ) 6-3 6-2\n",
      "  FP: third\n",
      "Sentence: 2 - Monica Seles ( U.S. ) beat Dally Randriantefy ( Madagascar )\n",
      "  FP: 2 - Monica Seles\n",
      "  FN: Monica Seles\n",
      "  FN: Madagascar\n",
      "Sentence: Add men 's singles , second round 12 - Todd Martin ( U.S. ) beat Andrea Gaudenzi ( Italy ) 6-3 6-2 6-2 Stefan Edberg ( Sweden ) beat Bernd Karbacher ( Germany ) 3-6 6-3 6-3 1-0 retired ( leg injury )\n",
      "  FP: second\n",
      "  FP: 12\n",
      "Sentence: BASEBALL - MAJOR LEAGUE STANDINGS AFTER THURSDAY 'S GAMES .\n",
      "  FP: BASEBALL - MAJOR LEAGUE STANDINGS AFTER THURSDAY 'S\n",
      "  FN: MAJOR LEAGUE\n",
      "Sentence: AMERICAN LEAGUE EASTERN DIVISION\n",
      "  FP: AMERICAN LEAGUE EASTERN\n",
      "  FN: AMERICAN LEAGUE EASTERN DIVISION\n",
      "Sentence: NEW YORK 74 59 .556 -\n",
      "  FP: 74 59\n",
      "  FN: NEW YORK\n",
      "Sentence: BALTIMORE 70 63 .526 4\n",
      "  FP: 70 63\n",
      "Sentence: BOSTON 69 65 .515 5 1/2\n",
      "  FP: 5 1/2\n",
      "  FP: 69 65\n",
      "Sentence: TORONTO 63 71 .470 11 1/2\n",
      "  FP: 11 1/2\n",
      "  FP: 63 71\n",
      "  FN: TORONTO\n",
      "Sentence: DETROIT 48 86 .358 26 1/2\n",
      "  FP: 26 1/2\n",
      "  FP: 48 86\n",
      "Sentence: CENTRAL DIVISION\n",
      "  FP: CENTRAL\n",
      "  FN: CENTRAL DIVISION\n",
      "Sentence: CLEVELAND 80 53 .602 -\n",
      "  FP: 53\n",
      "  FN: CLEVELAND\n",
      "Sentence: CHICAGO 71 64 .526 10\n",
      "  FP: 71 64\n",
      "  FP: 10\n",
      "Sentence: MINNESOTA 67 67 .500 13 1/2\n",
      "  FP: 67 67\n",
      "  FP: 13 1/2\n",
      "Sentence: MILWAUKEE 64 71 .474 17\n",
      "  FP: 17\n",
      "  FN: MILWAUKEE\n",
      "Sentence: KANSAS CITY 61 74 .452 20\n",
      "  FP: 61 74\n",
      "  FP: 20\n",
      "Sentence: WESTERN DIVISION\n",
      "  FN: WESTERN DIVISION\n",
      "Sentence: SEATTLE 70 63 .526 5\n",
      "  FP: 70 63\n",
      "Sentence: OAKLAND 64 72 .471 12 1/2\n",
      "  FP: 12 1/2\n",
      "Sentence: CALIFORNIA 62 72 .463 13 1/2\n",
      "  FP: 62 72 .463\n",
      "  FP: 13 1/2\n",
      "Sentence: CLEVELAND AT TEXAS\n",
      "  FN: CLEVELAND\n",
      "Sentence: NATIONAL LEAGUE EASTERN DIVISION\n",
      "  FP: NATIONAL LEAGUE EASTERN\n",
      "  FN: NATIONAL LEAGUE EASTERN DIVISION\n",
      "Sentence: ATLANTA 83 49 .629 -\n",
      "  FP: 83 49\n",
      "Sentence: MONTREAL 71 61 .538 12\n",
      "  FP: 12\n",
      "  FP: 71\n",
      "  FN: MONTREAL\n",
      "Sentence: FLORIDA 64 70 .478 20\n",
      "  FP: 20\n",
      "  FP: 64 70\n",
      "Sentence: NEW YORK 59 75 .440 25\n",
      "  FP: 25\n",
      "  FP: 59 75 .440\n",
      "  FN: NEW YORK\n",
      "Sentence: PHILADELPHIA 54 80 .403 30\n",
      "  FP: 30\n",
      "  FP: 54 80\n",
      "Sentence: CENTRAL DIVISION\n",
      "  FP: CENTRAL\n",
      "  FN: CENTRAL DIVISION\n",
      "Sentence: HOUSTON 72 63 .533 -\n",
      "  FP: 72 63\n",
      "Sentence: ST LOUIS 69 65 .515 2 1/2\n",
      "  FP: 2 1/2\n",
      "  FP: 69 65 .515\n",
      "  FP: ST\n",
      "  FN: ST LOUIS\n",
      "Sentence: CINCINNATI 66 67 .496 5\n",
      "  FP: 66 67 .496\n",
      "  FN: CINCINNATI\n",
      "Sentence: PITTSBURGH 56 77 .421 15\n",
      "  FP: 56 77\n",
      "  FP: 15\n",
      "  FN: PITTSBURGH\n",
      "Sentence: WESTERN DIVISION\n",
      "  FN: WESTERN DIVISION\n",
      "Sentence: SAN DIEGO 75 60 .556 -\n",
      "  FP: 75 60\n",
      "  FN: SAN DIEGO\n",
      "Sentence: LOS ANGELES 72 61 .541 2\n",
      "  FP: 2\n",
      "  FP: 72 61\n",
      "Sentence: COLORADO 70 65 .519 5\n",
      "  FP: 5\n",
      "Sentence: SAN DIEGO AT MONTREAL\n",
      "  FN: SAN DIEGO\n",
      "Sentence: SAN FRANCISCO AT NEW YORK\n",
      "  FN: SAN FRANCISCO\n",
      "Sentence: COLORADO AT ST LOUIS\n",
      "  FP: ST\n",
      "  FN: ST LOUIS\n",
      "Sentence: BASEBALL - MAJOR LEAGUE RESULTS THURSDAY .\n",
      "  FP: BASEBALL - MAJOR LEAGUE RESULTS THURSDAY\n",
      "  FN: MAJOR LEAGUE\n",
      "Sentence: DETROIT 4 Kansas City 1\n",
      "  FP: 4\n",
      "Sentence: Minnesota 6 MILWAUKEE 1\n",
      "  FP: 6\n",
      "  FN: MILWAUKEE\n",
      "Sentence: CALIFORNIA 14 New York 3\n",
      "  FP: 3\n",
      "  FP: 14\n",
      "Sentence: SEATTLE 9 Baltimore 6\n",
      "  FP: 6\n",
      "  FP: 9\n",
      "  FN: Baltimore\n",
      "Sentence: San Diego 3 NEW YORK 2\n",
      "  FP: 3\n",
      "  FN: NEW YORK\n",
      "Sentence: Chicago 4 HOUSTON 3\n",
      "  FP: 3\n",
      "  FN: HOUSTON\n",
      "Sentence: Cincinnati 18 COLORADO 7\n",
      "  FP: 18\n",
      "  FN: COLORADO\n",
      "Sentence: Atlanta 5 PITTSBURGH 1\n",
      "  FP: 1\n",
      "  FP: 5\n",
      "  FN: PITTSBURGH\n",
      "Sentence: Los Angeles 2 MONTREAL 1\n",
      "  FP: Los Angeles 2\n",
      "  FP: 1\n",
      "  FN: MONTREAL\n",
      "  FN: Los Angeles\n",
      "Sentence: Florida 10 ST LOUIS 9\n",
      "  FP: 10\n",
      "Sentence: TENNIS - TARANGO , O'BRIEN SPRING TWIN UPSETS UNDER THE LIGHTS .\n",
      "  FP: SPRING TWIN\n",
      "  FN: TARANGO\n",
      "  FN: O'BRIEN\n",
      "Sentence: Andre Agassi escaped disaster on Thursday but Wimbledon finalist MaliVai Washington and Marcelo Rios were not so fortunate on a night of upsets at the U.S. Open .\n",
      "  FP: Thursday\n",
      "  FP: a night\n",
      "  FP: the U.S. Open\n",
      "  FN: U.S. Open\n",
      "Sentence: The 11th-seeded Washington fell short of reprising his Wimbledon miracle comeback as he lost to red-hot wildcard Alex O'Brien 6-3 6-4 5-7 3-6 6-3 in a two hour 51 minute struggle on the Stadium court .\n",
      "  FP: Wimbledon\n",
      "  FP: 51 minute\n",
      "  FP: 6-4 5-7 3-6 6-3\n",
      "  FP: 11th-seeded\n",
      "  FP: two hour\n",
      "  FP: Alex O'Brien\n",
      "  FP: Stadium\n"
     ]
    }
   ],
   "source": [
    "df_p2=df_dev.iloc[:500,]\n",
    "error_report(df_p2,set(gold_spans(df_p2)),set(Pred_entity(df_p2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write a short text that summarises the errors that you observed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_label(entities, df):  # label_, string\n",
    "    #res = []\n",
    "    for s, b, e in entities:\n",
    "        entity_ = \" \".join(df.loc[df_dev.sentence_id==s, \"sentence\"].values[0].split()[b:e])\n",
    "        ents = nlp(entity_).ents\n",
    "        yield ents[0].label_ if len(ents)!=0 else \"None\"  # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = list(set(gold_spans(df_dev)) - set(Pred_entity(df_dev))   )\n",
    "FP = list(set(Pred_entity(df_dev)) - set(gold_spans(df_dev)))\n",
    "FP_lab = entity_label(FP, df_dev)\n",
    "FN_lab = entity_label(FN, df_dev)\n",
    "gold_lab = entity_label(gold_spans(df_dev), df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP label counts:\n",
      "CARDINAL       1554\n",
      "DATE            746\n",
      "ORG             281\n",
      "ORDINAL         226\n",
      "None            223\n",
      "PERSON          128\n",
      "GPE             107\n",
      "TIME             68\n",
      "MONEY            55\n",
      "QUANTITY         50\n",
      "PERCENT          47\n",
      "NORP             44\n",
      "EVENT            23\n",
      "LOC               7\n",
      "LAW               7\n",
      "PRODUCT           3\n",
      "FAC               3\n",
      "WORK_OF_ART       1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "FN label counts:\n",
      "None        891\n",
      "PERSON      332\n",
      "ORG         264\n",
      "GPE         216\n",
      "NORP         64\n",
      "EVENT        21\n",
      "LOC          10\n",
      "DATE          8\n",
      "LANGUAGE      4\n",
      "CARDINAL      4\n",
      "FAC           3\n",
      "PRODUCT       1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Golden label counts:\n",
      "GPE            1655\n",
      "PERSON         1515\n",
      "None           1286\n",
      "ORG             727\n",
      "NORP            565\n",
      "EVENT            57\n",
      "LOC              49\n",
      "LANGUAGE         19\n",
      "FAC              13\n",
      "DATE             11\n",
      "CARDINAL          9\n",
      "PRODUCT           8\n",
      "WORK_OF_ART       2\n",
      "ORDINAL           1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"FP label counts:\")\n",
    "print(pd.DataFrame({\"label\": FP_lab}).label.value_counts())\n",
    "print(\"\\nFN label counts:\")\n",
    "print(pd.DataFrame({\"label\": FN_lab}).label.value_counts())\n",
    "print(\"\\nGolden label counts:\")\n",
    "print(pd.DataFrame({\"label\": gold_lab}).label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data above, we could find that most error's lable are \"CARDINAL\", \"DATE\", \"ORDINAL\",\"TIME\", \"QUANTITY\", \"MONEY\", and \"PERCENT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the insights from your error analysis to improve the automated prediction that you implemented in Problem&nbsp;2. While the best way to do this would be to [update spaCy&rsquo;s NER model](https://spacy.io/usage/linguistic-features#updating) using domain-specific training data, for this lab it suffices to write code to post-process the output produced by spaCy. To filter out specific labels it is useful to know the named entity label scheme, which can be found in the [model's documentation](https://spacy.io/models/en#en_core_web_sm). You should be able to improve the F1 score from Problem&nbsp;2 by at last 15 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to improve the span prediction from Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pred_entity_improved(df,lab_filtered):\n",
    "  for row in df.iloc[:, [0,1]].drop_duplicates().itertuples(index=False): # .drop_duplicates()\n",
    "    for ent in nlp(row[1]).ents:\n",
    "      if not ent.label_ in lab_filtered:\n",
    "        yield (row[0], ent.start, ent.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 85.27%\n",
      "The recall is 69.24%\n",
      "The F1-value is 76.42%\n"
     ]
    }
   ],
   "source": [
    "lab_filtered = [\"CARDINAL\", \"DATE\", \"ORDINAL\",\n",
    "                \"TIME\", \"QUANTITY\", \"MONEY\", \"PERCENT\"]\n",
    "evaluation_report(set(gold_spans(df_dev)),set(Pred_entity_improved(df_dev,lab_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that you achieve the performance goal by reporting the evaluation measures that you implemented in Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, we ask you to store the outputs of the improved named entity recognizer on the development data in a new data frame. This new frame should have the same layout as the original data frame for the development data that you loaded above, but should contain the *predicted* start and end positions for each token span, rather than the gold positions. As the `label` of each span, you can use the special value `--NME--`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to store the predicted spans in a new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-001</td>\n",
       "      <td>LONDON 1996-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0946-003</td>\n",
       "      <td>Their stay on top , though , may be short-live...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>1161-004</td>\n",
       "      <td>There will be both buying and selling pressure...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>1161-005</td>\n",
       "      <td>Broker Khurshid Alam said : \" The market senti...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>1161-009</td>\n",
       "      <td>The DSE all share price index closed 2.73 poin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4805 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                           sentence  beg  end  \\\n",
       "0       0946-001                                  LONDON 1996-08-30    0    1   \n",
       "1       0946-002  West Indian all-rounder Phil Simmons took four...    0    2   \n",
       "2       0946-002  West Indian all-rounder Phil Simmons took four...    3    5   \n",
       "3       0946-002  West Indian all-rounder Phil Simmons took four...   12   13   \n",
       "4       0946-003  Their stay on top , though , may be short-live...   13   14   \n",
       "...          ...                                                ...  ...  ...   \n",
       "4800    1161-004  There will be both buying and selling pressure...   12   14   \n",
       "4801    1161-005  Broker Khurshid Alam said : \" The market senti...    0    3   \n",
       "4802    1161-007  Brokers said blue chips like IDLC , Bangladesh...    7    9   \n",
       "4803    1161-007  Brokers said blue chips like IDLC , Bangladesh...   13   15   \n",
       "4804    1161-009  The DSE all share price index closed 2.73 poin...    1    2   \n",
       "\n",
       "        label  \n",
       "0     --NME--  \n",
       "1     --NME--  \n",
       "2     --NME--  \n",
       "3     --NME--  \n",
       "4     --NME--  \n",
       "...       ...  \n",
       "4800  --NME--  \n",
       "4801  --NME--  \n",
       "4802  --NME--  \n",
       "4803  --NME--  \n",
       "4804  --NME--  \n",
       "\n",
       "[4805 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_ent = pd.DataFrame(list(Pred_entity_improved(df_dev,lab_filtered) ), columns=[\"sentence_id\", \"beg\", \"end\"])\n",
    "df_pred_ent[\"sentence\"] = df_pred_ent.sentence_id.apply(lambda x: df_dev.loc[df_dev.sentence_id==x].sentence.values[0])\n",
    "df_pred_ent[\"label\"] = [\"--NME--\"]* df_pred_ent.shape[0]\n",
    "df_pred_ent = df_pred_ent.loc[:, [\"sentence_id\", \"sentence\", \"beg\", \"end\", \"label\"]]\n",
    "df_pred_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method for predicting mention spans, we turn to the task of **entity linking**, which amounts to predicting the knowledge base entity that is referenced by a given mention. In our case, for each span we want to predict the Wikipedia page that this mention references.\n",
    "\n",
    "Start by extending the generator function that you implemented in Problem&nbsp;2 to labelled spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_mentions(df):\n",
    "    \"\"\"Yield the gold-standard mentions in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and entity label of each span.\n",
    "    \"\"\"\n",
    "    # TODO: Replace the next line with your own code\n",
    "    for row in df.iloc[:,[0,2,3,4]].itertuples(index=False, name=None):\n",
    "        yield row\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive baseline for entity linking on our data set is to link each mention span to the Wikipedia page name that we get when we join the tokens in the span by underscores, as is standard in Wikipedia page names. Suppose, for example, that a span contains the two tokens\n",
    "\n",
    "    Jimi Hendrix\n",
    "\n",
    "The baseline Wikipedia page name for this span would be\n",
    "\n",
    "    Jimi_Hendrix\n",
    "\n",
    "Implement this naive baseline and evaluate its performance. Print the evaluation measures that you implemented in Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here and in the remainder of this lab, you should base your entity predictions on the predicted spans that you computed in Problem&nbsp;3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-001</td>\n",
       "      <td>LONDON 1996-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>LONDON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>West_Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Phil_Simmons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Leicestershire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0946-003</td>\n",
       "      <td>Their stay on top , though , may be short-live...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Essex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>1161-004</td>\n",
       "      <td>There will be both buying and selling pressure...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>Shakil_Rizvi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>1161-005</td>\n",
       "      <td>Broker Khurshid Alam said : \" The market senti...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Broker_Khurshid_Alam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>Bangladesh_Lamps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>Atlas_Bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>1161-009</td>\n",
       "      <td>The DSE all share price index closed 2.73 poin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>DSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4805 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                           sentence  beg  end  \\\n",
       "0       0946-001                                  LONDON 1996-08-30    0    1   \n",
       "1       0946-002  West Indian all-rounder Phil Simmons took four...    0    2   \n",
       "2       0946-002  West Indian all-rounder Phil Simmons took four...    3    5   \n",
       "3       0946-002  West Indian all-rounder Phil Simmons took four...   12   13   \n",
       "4       0946-003  Their stay on top , though , may be short-live...   13   14   \n",
       "...          ...                                                ...  ...  ...   \n",
       "4800    1161-004  There will be both buying and selling pressure...   12   14   \n",
       "4801    1161-005  Broker Khurshid Alam said : \" The market senti...    0    3   \n",
       "4802    1161-007  Brokers said blue chips like IDLC , Bangladesh...    7    9   \n",
       "4803    1161-007  Brokers said blue chips like IDLC , Bangladesh...   13   15   \n",
       "4804    1161-009  The DSE all share price index closed 2.73 poin...    1    2   \n",
       "\n",
       "                     label  \n",
       "0                   LONDON  \n",
       "1              West_Indian  \n",
       "2             Phil_Simmons  \n",
       "3           Leicestershire  \n",
       "4                    Essex  \n",
       "...                    ...  \n",
       "4800          Shakil_Rizvi  \n",
       "4801  Broker_Khurshid_Alam  \n",
       "4802      Bangladesh_Lamps  \n",
       "4803      Atlas_Bangladesh  \n",
       "4804                   DSE  \n",
       "\n",
       "[4805 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_baseline(df):\n",
    "  df.label = df.apply(lambda row: \"_\".join(row.sentence.split()[row.beg:row.end]), axis=1)\n",
    "  return(df)    #for row in df.iloc[:,[0,2,3,4]].itertuples(index=False, name=None): \n",
    "\n",
    "df_naive = naive_baseline(df_pred_ent)\n",
    "df_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 31.70%\n",
      "The recall is 25.74%\n",
      "The F1-value is 28.41%\n"
     ]
    }
   ],
   "source": [
    "evaluation_report(set(gold_mentions(df_dev)), set(gold_mentions(df_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Extending the training data using the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art approaches to entity linking exploit information in knowledge bases. In our case, where Wikipedia is the knowledge base, one particularly useful type of information are links to other Wikipedia pages. In particular, we can interpret the anchor texts (the highlighted texts that you click on) as mentions of the entities (pages) that they link to. This allows us to harvest long lists of mention–entity pairings.\n",
    "\n",
    "The following cell loads a data frame summarizing anchor texts and page references harvested from the first paragraphs of the English Wikipedia. The data frame also contains all entity mentions in the training data (but not the development or the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('kb.tsv.bz2', 'rt', encoding='utf-8') as source:\n",
    "    df_kb = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what information is available in this data, the following cell shows the entry for the anchor text `Sweden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17436</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.985768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17437</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>0.014173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17438</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_men's_national_ice_hockey_team</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention                                 entity      prob\n",
       "17436  Sweden                                 Sweden  0.985768\n",
       "17437  Sweden          Sweden_national_football_team  0.014173\n",
       "17438  Sweden  Sweden_men's_national_ice_hockey_team  0.000059"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.loc[df_kb.mention == 'Sweden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each row of the data frame contains a pair $(m, e)$ of a mention $m$ and an entity $e$, as well as the conditional probability $P(e|m)$ for mention $m$ referring to entity $e$. These probabilities were estimated based on the frequencies of mention–entity pairs in the knowledge base. The example shows that the anchor text &lsquo;Sweden&rsquo; is most often used to refer to the entity [Sweden](http://en.wikipedia.org/wiki/Sweden), but in a few cases also to refer to Sweden&rsquo;s national football and ice hockey teams. Note that references are sorted in decreasing order of probability, so that the most probable pairing come first.\n",
    "\n",
    "Implement an entity linking method that resolves each mention to the most probable entity in the data frame. If the mention is not included in the data frame, you can predict the generic label `--NME--`. Print the precision, recall, and F1 of your method using the function that you implemented for Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the \"most probable entity\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_prob_ent(df_ent, df_kb):  # high probability entity\n",
    "  for row in df_ent.itertuples(index=False):\n",
    "    mention = \" \".join(row.sentence.split()[row.beg:row.end])\n",
    "    try:\n",
    "      yield df_kb.loc[df_kb.loc[df_kb[\"mention\"] == mention, \"prob\"].idxmax(), \"entity\"] \n",
    "    except:\n",
    "      yield \"--NME--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-001</td>\n",
       "      <td>LONDON 1996-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>West_Indies_cricket_team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Phil_Simmons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Leicestershire_County_Cricket_Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0946-003</td>\n",
       "      <td>Their stay on top , though , may be short-live...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Essex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>1161-004</td>\n",
       "      <td>There will be both buying and selling pressure...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>1161-005</td>\n",
       "      <td>Broker Khurshid Alam said : \" The market senti...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>1161-009</td>\n",
       "      <td>The DSE all share price index closed 2.73 poin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Dhaka_Stock_Exchange</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4805 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                           sentence  beg  end  \\\n",
       "0       0946-001                                  LONDON 1996-08-30    0    1   \n",
       "1       0946-002  West Indian all-rounder Phil Simmons took four...    0    2   \n",
       "2       0946-002  West Indian all-rounder Phil Simmons took four...    3    5   \n",
       "3       0946-002  West Indian all-rounder Phil Simmons took four...   12   13   \n",
       "4       0946-003  Their stay on top , though , may be short-live...   13   14   \n",
       "...          ...                                                ...  ...  ...   \n",
       "4800    1161-004  There will be both buying and selling pressure...   12   14   \n",
       "4801    1161-005  Broker Khurshid Alam said : \" The market senti...    0    3   \n",
       "4802    1161-007  Brokers said blue chips like IDLC , Bangladesh...    7    9   \n",
       "4803    1161-007  Brokers said blue chips like IDLC , Bangladesh...   13   15   \n",
       "4804    1161-009  The DSE all share price index closed 2.73 poin...    1    2   \n",
       "\n",
       "                                   label  \n",
       "0                                 London  \n",
       "1               West_Indies_cricket_team  \n",
       "2                           Phil_Simmons  \n",
       "3     Leicestershire_County_Cricket_Club  \n",
       "4                                  Essex  \n",
       "...                                  ...  \n",
       "4800                             --NME--  \n",
       "4801                             --NME--  \n",
       "4802                             --NME--  \n",
       "4803                             --NME--  \n",
       "4804                Dhaka_Stock_Exchange  \n",
       "\n",
       "[4805 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels = list(H_prob_ent(df_pred_ent, df_kb))\n",
    "df_pred_ent.label = new_labels\n",
    "df_pred_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 65.79%\n",
      "The recall is 53.42%\n",
      "The F1-value is 58.96%\n"
     ]
    }
   ],
   "source": [
    "evaluation_report(set(gold_mentions(df_dev)), set(gold_mentions(df_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Context-sensitive disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the entity mention &lsquo;Lincoln&rsquo;. The most probable entity for this mention turns out to be [Lincoln, Nebraska](http://en.wikipedia.org/Lincoln,_Nebraska); but in pages about American history, we would be better off to predict [Abraham Lincoln](http://en.wikipedia.org/Abraham_Lincoln). This suggests that we should try to disambiguate between different entity references based on the textual context on the page from which the mention was taken. Your task in this last problem is to implement this idea.\n",
    "\n",
    "Set up a dictionary that contains, for each mention $m$ that can refer to more than one entity $e$, a separate Naive Bayes classifier that is trained to predict the correct entity $e$, given the textual context of the mention. As the prior probabilities of the classifier, choose the probabilities $P(e|m)$ that you used in Problem&nbsp;5. To let you estimate the context-specific probabilities, we have compiled a data set with mention contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('contexts.tsv.bz2') as source:\n",
    "    df_contexts = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame contains, for each ambiguous mention $m$ and each knowledge base entity $e$ to which this mention can refer, up to 100 randomly selected contexts in which $m$ is used to refer to $e$. For this data, a **context** is defined as the 5 tokens to the left and the 5 tokens to the right of the mention. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>UEFA_Champions_League</td>\n",
       "      <td>Cup twice the first in @ and the second in 1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>FIFA_World_Cup</td>\n",
       "      <td>America 1975 and during the @ and 1978 World C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>Manolo represented Spain at the @</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>Hašek represented Czechoslovakia at the @ and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>renovations in 1989 for the @ The present capa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mention                 entity  \\\n",
       "0            1970  UEFA_Champions_League   \n",
       "1            1970         FIFA_World_Cup   \n",
       "2  1990 World Cup    1990_FIFA_World_Cup   \n",
       "3  1990 World Cup    1990_FIFA_World_Cup   \n",
       "4  1990 World Cup    1990_FIFA_World_Cup   \n",
       "\n",
       "                                             context  \n",
       "0    Cup twice the first in @ and the second in 1983  \n",
       "1  America 1975 and during the @ and 1978 World C...  \n",
       "2                 Manolo represented Spain at the @   \n",
       "3  Hašek represented Czechoslovakia at the @ and ...  \n",
       "4  renovations in 1989 for the @ The present capa...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in each context, the position of the mention is indicated by the `@` symbol.\n",
    "\n",
    "From this data frame, it is easy to select the data that you need to train the classifiers – the contexts and corresponding entities for all mentions. To illustrate this, the following cell shows how to select all contexts that belong to the mention &lsquo;Lincoln&rsquo;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41465    Nebraska Concealed Handgun Permit In @ municip...\n",
       "41466    Lazlo restaurants are located in @ and Omaha C...\n",
       "41467    California Washington Overland Park Kansas @ N...\n",
       "41468    City Missouri Omaha Nebraska and @ Nebraska It...\n",
       "41469    by Sandhills Publishing Company in @ Nebraska USA\n",
       "                               ...                        \n",
       "41609                                      @ Leyton Orient\n",
       "41610                    English division three Swansea @ \n",
       "41611    league membership narrowly edging out @ on goa...\n",
       "41612                                          @ Cambridge\n",
       "41613                                                   @ \n",
       "Name: context, Length: 149, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts.context[df_contexts.mention == 'Lincoln']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the context-sensitive disambiguation method and evaluate its performance. Here are some more hints that may help you along the way:\n",
    "\n",
    "**Hint 1:** The prior probabilities for a Naive Bayes classifier can be specified using the `class_prior` option. You will have to provide the probabilities in the same order as the alphabetically sorted class (entity) names.\n",
    "\n",
    "**Hint 2:** Not all mentions in the knowledge base are ambiguous, and therefore not all mentions have context data. If a mention has only one possible entity, pick that one. If a mention has no entity at all, predict the `--NME--` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code here to implement the context-sensitive disambiguation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = df_contexts.mention.unique()\n",
    "nb_mention_train_models = {}\n",
    "\n",
    "for m in mentions:\n",
    "    docs = df_contexts.loc[df_contexts.mention==m, \"context\"]\n",
    "    entities = df_contexts.loc[df_contexts.mention==m, \"entity\"]\n",
    "    prior = df_kb.loc[df_kb.mention==m].sort_values(by=\"entity\").prob.values\n",
    "    pipeline = make_pipeline(CountVectorizer(),\n",
    "                            MultinomialNB(class_prior=prior))\n",
    "    pipeline.fit(docs, entities)\n",
    "    nb_mention_train_models[m] = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_pred(df_ent, df_kb, nb_mention_train_models):\n",
    "  #--------------------------------------------------  \n",
    "  for row in df_ent.itertuples(index=False):\n",
    "      mention = \" \".join(row.sentence.split()[row.beg:row.end])\n",
    "      if mention in nb_mention_train_models.keys():\n",
    "          yield nb_mention_train_models[mention].predict([row.sentence])[0]\n",
    "      elif mention in df_kb.mention.values:\n",
    "          yield df_kb.loc[df_kb.loc[df_kb[\"mention\"] == mention, \"prob\"].idxmax(), \"entity\"]\n",
    "      else:\n",
    "          yield \"--NME--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-001</td>\n",
       "      <td>LONDON 1996-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Phil_Simmons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Leicestershire_County_Cricket_Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0946-003</td>\n",
       "      <td>Their stay on top , though , may be short-live...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Essex_County_Cricket_Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>1161-004</td>\n",
       "      <td>There will be both buying and selling pressure...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>1161-005</td>\n",
       "      <td>Broker Khurshid Alam said : \" The market senti...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>1161-007</td>\n",
       "      <td>Brokers said blue chips like IDLC , Bangladesh...</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>1161-009</td>\n",
       "      <td>The DSE all share price index closed 2.73 poin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Dhaka_Stock_Exchange</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4805 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                           sentence  beg  end  \\\n",
       "0       0946-001                                  LONDON 1996-08-30    0    1   \n",
       "1       0946-002  West Indian all-rounder Phil Simmons took four...    0    2   \n",
       "2       0946-002  West Indian all-rounder Phil Simmons took four...    3    5   \n",
       "3       0946-002  West Indian all-rounder Phil Simmons took four...   12   13   \n",
       "4       0946-003  Their stay on top , though , may be short-live...   13   14   \n",
       "...          ...                                                ...  ...  ...   \n",
       "4800    1161-004  There will be both buying and selling pressure...   12   14   \n",
       "4801    1161-005  Broker Khurshid Alam said : \" The market senti...    0    3   \n",
       "4802    1161-007  Brokers said blue chips like IDLC , Bangladesh...    7    9   \n",
       "4803    1161-007  Brokers said blue chips like IDLC , Bangladesh...   13   15   \n",
       "4804    1161-009  The DSE all share price index closed 2.73 poin...    1    2   \n",
       "\n",
       "                                   label  \n",
       "0                                 London  \n",
       "1                              Caribbean  \n",
       "2                           Phil_Simmons  \n",
       "3     Leicestershire_County_Cricket_Club  \n",
       "4              Essex_County_Cricket_Club  \n",
       "...                                  ...  \n",
       "4800                             --NME--  \n",
       "4801                             --NME--  \n",
       "4802                             --NME--  \n",
       "4803                             --NME--  \n",
       "4804                Dhaka_Stock_Exchange  \n",
       "\n",
       "[4805 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels_again = list(  nb_pred(df_pred_ent, df_kb, nb_mention_train_models)   )\n",
    "df_pred_ent.label = new_labels_again\n",
    "df_pred_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see a small (around 1&nbsp;unit) increase in both precision, recall, and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 67.37%\n",
      "The recall is 54.71%\n",
      "The F1-value is 60.38%\n"
     ]
    }
   ],
   "source": [
    "evaluation_report(set(gold_mentions(df_dev)), set(gold_mentions(df_pred_ent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following reflection questions are questions that you could be asked in the oral exam. Try to answer each of them in the form of a short text and enter it in the cell below. You will get feedback on your answers from your lab assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RQ 5.1:** In Problem&nbsp;3, you did an error analysis on the task of recognizing text spans mentioning named entities. Summarize your results. Pick one type of error that you observed. How could you improve the model&rsquo;s performance on this type of error? What resources (such as domain knowledge, data, compute) would you need to implement this improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results in Prolbem 3, we could find that most error's lable are \"CARDINAL\", \"DATE\", \"ORDINAL\",\"TIME\", \"QUANTITY\", \"MONEY\", and \"PERCENT\", and the FP and FN are too high. In this case, if we want to imporve the performance of FP, we may need to check the data for ensuring the data is balanced, if it is not ,we need to balanced the data first than do the training again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RQ 5.2:** Thinking back about Problem&nbsp;6, explain what the word *context* refers to in the task addressed there, and how context can help to disambiguate between different entities. Suggest other types of context that you could use for disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Cup twice the first in @ and the second in 1983\n",
       "1    America 1975 and during the @ and 1978 World C...\n",
       "2                   Manolo represented Spain at the @ \n",
       "3    Hašek represented Czechoslovakia at the @ and ...\n",
       "4    renovations in 1989 for the @ The present capa...\n",
       "Name: context, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts['context'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word context refers to the special event. It shows the main chararcter of the different entities. Other types of context we could use for disambiguation is the brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RQ 5.3:** One type of entity mentions that we did not cover explicitly in this lab are pronouns. As an example, consider the sentence pair *Ruth Bader Ginsburg was an American jurist*. *She served as an associate justice of the Supreme Court from 1993 until her death in 2020*. What facts would you want to extract from this sentence pair? How do pronouns make fact extraction hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruth Bader Ginsburg, who as an American jurist served as an associate justice of the Supreme Court from 1993 until her death in 2020. For sometimes the pronouns need the relative context to confirm the true target they assume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Enter your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This was the last lab in the Text Mining course. Congratulations! 🥳**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
