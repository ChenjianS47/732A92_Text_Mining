---
title: Sentiment classification in the review of Genshin Impact
subtitle: "732A92 Text Mining"
author: Chenjian Shi (chesh532)
date: "Aug 16,2022"
abstract: Nowadays, video games are a vital part of the entertainment industry, and mobile games play an essential role. Recently, a mobile game named Genshin Impact has become increasingly popular and achieved many international awards. People talk about this game on Twitter and Reddit. Therefore, we want to do the sentiment analysis on the comments on Twitter and Reddit. However, we do not have labeled data of comments on Twitter and Reddit, so in this paper, we would like to use the reviews of Genshin Impact from the Google play store to build a sentiment classifier for classifying the data from Twitter. Finally, we found that the classifier based on the BERT has the best performance among the models in this paper. Using the BERT model to predict Twitter's data, we find that most people stay neutral but tend to give it a positive comment.
output: 
  pdf_document: 
    toc: yes
    number_sections: yes
    extra_dependencies: ["flafter"]
    latex_engine: xelatex
bibliography: Reference.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( echo=FALSE)
library(reticulate)
Sys.setenv(RETICULATE_PYTHON = "E:\\Anaconda3\\envs\\tf")
use_condaenv(condaenv = "tf")
library(data.table)
library("readxl")
library(tidyverse)
```

\newpage

# Introduction

Nowadays, video games are a vital part of the entertainment industry, and more and more people choose this way to relax. Meanwhile, people would like to play mobile games instead of traditional PC games or console games due to mobile phones' convenience and increasing performance. So they could play the games they like anywhere they like, for example, in the subway, or the Cafe, instead of playing games in a fixed place. Genshin impact, first published in Sep 2020, has become more popular. According to the report from BBC news[@bbc_news], Genshin Impact stands at the top of mobile games, which earns $2 billion after 'unheard of' success in the first year. Moreover, this game was recently awarded many international awards in the field, like The Game Award for Best Mobile Game, Etc. [@TGA]. A newborn game could rarely get these achievements. 

As we know, Twitter and Reddit are the most famous discussion places for people. So we could try to learn why Genshin Impact is so popular with people by analyzing the comments on Twitter and Reddit. However, currently, we do not have a data set with a sentiment label to analyze, which means it is hard for us to learn the reason from a comment since we do not know whether it is positive. So we need to find a way to get a model that could do the sentiment classification for the comments on Twitter and Reddit. Fortunately, we have the Google play store. The Google play store is one of the most popular video game stores on mobile phones. Therefore, massive reviews of games about the players' views on the games could be used to study the player's sentiment classification. Besides, we could use its score system as the label.


In this paper, we will try to apply these several machine learning and text mining skills to analyze the reviews from the Google play store to train the Sentiment classifier and then use the best classifier to do the sentiment classification on Twitter's data.

# Theory. 

## Data Balancing

### Synthetic Minority Over-sampling Technique (SMOTE)

Usually, the data in the natural lie is unbalanced, which means the number of each class/label is different. We need to balance the data to meet the requirement of the models, which requires the data has an equal number per class of labels. In this case, the Synthetic Minority Over-sampling Technique (SMOTE)[@SMOTE] would be used to balance the data. It may be the most popular oversampling method to do the data balancing. SMOTE could not only over-sampling the minority class but also the under-sampling of the majority class. The algorithm of SMOTE is processing as below, 

For each sample, $x$ in the minority class, use the Euclidean distance as the standard to calculate its distance to all samples in the minority class sample set $S$ and get its k nearest neighbors. 

Then a sampling ratio is set according to the sample imbalance ratio to help determine the sampling ratio N. For each minority class sample $x$, several samples are randomly selected from its k nearest neighbors, assuming that the selected nearest neighbor is $xn$.

At last, for each randomly selected $xn$, then construct a new sample with the original sample according to the following formula:
$x_{new}=x+rand(0,1)*|x-xn|$

However, there would create bias in the data sets.

## Machine Learning Models

### Non-deep Learning

\textbf{Linear Support Vector Classification (LSVC)} is a kind of supervised learning model and related learning algorithm for analyzing data in classification and regression analysis.[@cortes1995support] The LSVC implements a “one-vs-the-rest” multi-class strategy, thus training n classes.[@scikit-learn]

![Linear Support Vector Classification formula]("./Formula-LSVC.PNG")

\textbf{Logistic regression (LR)} is a generalized linear regression analysis model, often used in data mining, automatic disease diagnosis, economic forecasting, and other fields.[@scikit-learn]

![Logistic Regression formula]("./Formula-LR.PNG")


### Deep Learning

\textbf{Bidirectional Long Short-Term Memory (BiLSTM) :}

Long short-term memory (LSTM) is a particular recurrent neural network (RNN), mainly to solve the problem of gradient disappearance and gradient explosion during long sequence training. Simply put, LSTM can perform better in longer sequences than ordinary RNNs.

The LSTM model is composed of the input word $x_t$ at time t, the cell state $C_t$, the temporary cell state $\hat{C_t}$, the hidden layer state $h_t$, the forgetting gate $f_t$, the memory gate $i_t$, and the output gate $o_t$. For the calculation process of LSTM, first, forgetting the information in the cell state and memorizing new information, the valuable information for subsequent calculations is transmitted. Then discard useless information, and the hidden layer state $h_t$ is output at each step. Besides, the forgetting, memory, and output are controlled by the forgetting gate $f_t$, memory gate $i_t$, and the output gate $o_t$, which are calculated by the last time hidden layer state $h_{t-1}$ and current input $X_t$.[@LSTM]


![LSTM-theory]("./Theory-LSTM-cell.jpg")

[@LSTM-cell]

Bidirectional Long Short-Term Memory (BiLSTM) comprises forwarding LSTM and backward LSTM. [@BiLSTM]

For example, input "I", "Love", "You" in turn to get three vectors $[h_{L0},h_{L1} ,h_{L2} ]$. Enter "you", "love", and "me" in turn to get three vectors $[h_{R0},h_{R1} ,h_{R2} ]$. Finally, the forward and backward hidden vectors are spliced to obtain$[(h_{L0}, h_{R0}),(h_{L1},h_{R1}) ,(h_{L2},h_{R2}) ]$, namely $[h_{0},h_{1} ,h_{2} ]$

For sentiment classification tasks, the representation of the sentences we use is often $[(h_{L2},h_{R2}) ]$. Because it contains all the forward and backward information

![BiLSTM]("./Theory-BiLSTM.png")

[@BiLSTM]

\textbf{Bidirectional Encoder Representations from Transformers (BERT) :}

BERT, a transformer-based bidirectional encoding representation, is a pre-training model. The two tasks during model training are to predict the words masked in the sentence and determine whether the two input sentences are upper and lower. After the pre-trained BERT model is added to the corresponding network according to the specific task, the downstream tasks of NLP can be completed, such as text classification, machine translation, Etc.[@vaswani2017attention]

Although BERT is based on the transformer, it only uses the encoder part of the transformer. Its overall framework is formed by stacking the encoders of multiple layers of transformers. The encoder of each layer is composed of a layer of muti-head-attention and a layer of feed-forward. The large model has 24 layers, each layer has 16 attention, and the miniature model has 12 layers. Each layer has 12 attention. The main role of each attention is to re-encode the target word through the relevance of the target word to all words in the sentence. Therefore, calculating each attention includes three steps: calculating the correlation between words, normalizing the correlation, and obtaining the encoding of the target word through the weighted summation of the correlation and the encoding of all words.

When calculating the correlation between words through attention, first linearly transform the input sequence vector (512*768) through three weight matrices, respectively generate three new sequence vectors of query, key, and value, and use each word. The query vector is multiplied with the key vector of all words in the sequence to obtain the correlation between words, and then this correlation is normalized by softmax. Next, the normalized weight is summed with the value weight. Finally, get a new encoding for each word.

![BERT-theory]("./Theory-BERT.PNG")

(Figure from [@vaswani2017attention])

In BERT, the input vector is summed by three different embeddings, namely[@BERT-input]:

Wordpiece embedding: The vector representation of the word itself. WordPiece refers to the division of words into a limited set of standard subword units, which can compromise words' effectiveness and the characters' flexibility.

Positional Embedding: Encodes the positional information of words into feature vectors. Because our network structure does not have RNN or LSTM, the position information of the sequence cannot be obtained, so the position embedding needs to be built. There are two ways to build positional embeddings: BERT initializes a positional embedding and then learns it through training, while Transformer builds positional embeddings by formulating rules.

Segment Embedding: A vector representation used to distinguish two sentences. It is used to distinguish asymmetric sentences, such as questions and answers.

The input of the BERT model is wordpiece token embedding + segment embedding + position embedding, as shown in the figure:

\newpage

![BERT-Input]("./Theory-BERT-EMB.PNG")

(Figure from [@BERT-input])


### Metrics

According to the task goal, we have select 3 differernt kinds of metrics from both scikit-learn [@scikit-learn] and tensorflow[@tensorflow2015-whitepaper]

  - Kappa score: The score lies in the range [-1, 1]. A score of -1 represents complete disagreement between two raters, whereas a score of 1 represents the complete agreement between the two raters. A score of 0 means agreement by chance.
  
  - fbeta-score: It is the weighted harmonic mean of precision and recall. Output range is [0, 1]. Works for both multi-class and multi-label classification. In order to compare the result between non-deep learning models and deep learning models, here we set the $\beta = 1$ in the fbeta-score so that the fbeta-score would be theoretically the same as the f1-score.
  
  - Area Under the ROC Curve (AUC): AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.[@AUC]

\newpage

# Data. 

## Data access

### Google Play

The data was scraped from the reviews of Genshin Impact on Google play[@googleplay] on Jan 09th, 2022, by python script, which is implemented from a package named google-play-scraper[@google-play-scraper]. According to the aim of this report, only choose the data of "Time," "Score," and "Content," which stand for the date, content, and the score of one review, and save them as Excel files. Besides, the score range in the Google play store is from 1 to 5, which could be regarded as a different attitude to things, like 1 (Extremely Negative), 2 (Negative), 3 (Neutral), 4 (Positive), 5 (Extremely Positive). The data we have is like below:

```{r}
print(read_excel("data_38k_us.xlsx"))
```

### Twitter

The data from Twitter was scraped by Twint, a package used for scrapping the data with specific contents from Twitter[@Twint]. According to the goal, we collected tweets with the tag "GenshinImpact" the data date is selected from 2022-01-09 to 2022-01-10. However, due to the limitation of the computational resources, we only collect 1522 rows of data for later usage. The data we have is like below:

```{r}
print(read_excel("valid_data.xlsx"))
```
We could find that the language in the Tweets is not so 'English', which contains some non-English word and some abbr. In this case, this data cannot be used in directly, but need a cleaning.


## Data Overview

### Google Play

Due to the limited resources or other cloud platforms like Colab, it is hard to use all these data since it takes about 10 hours to clean 3000 records. So, the data had to be sampled according to the original data's distribution with a ratio rate of 10%. The sampled data would be split into training and test sets. This data size for the training model may not be sufficient, leading to a low-performance model.

Wordcloud[@oesper2011wordcloud] and matplotlib[@matplotlib] would be used for data visualization.


![The Count Per Class of Labels for train data]("./Class_label-train.png"){width=70%}

![The wordcloud per class of labels for train data]("./Word_train.png")

\newpage


![The Count Per Class of Labels for test data]("./Class_label-test.png"){width=70%}

![The wordcloud per class of labels for test data]("./Word_test.png")

According to the figures above, we could find that the train data are imbalanced, which is necessary to balance before training. However, this data balancing would make bias to the results.

\newpage

### Twitter

![The wordcloud of Twitter's Data]("./Word_valid.png")

## Data Cleaning

In practice, the most important and troubling part of deploying a machine learning model might be data cleaning. When deploying a machine learning model, the choice of model and the parameter adjustment depends on the objective performance of the model, which usually has standard rules to follow. However, the data we used in the model cannot be changed if the performance is not good. Besides, if the data is of low quality, for example, the data size is too small, or the label/features are not correctly written in the data sets, all these would lead to bad model performance.

As for the data we use here, the content contains much informal spelling or abbreviation. Besides, even if we set the language to 'en,' there may still be non-English words or emojis. So to get a higher quality of data, we would use the way introduced below to do the data cleaning:

a) Contextual Spell Check
It is a spell check package [@spell_check] from the spacy universe, which uses the BERT model to correct the spelling of the words. So it could be added to the spacy pipeline. Here the doc._.outcome_spellCheck would be used as the correct content after the process. Besides, it could also be used for language detection in a way.


b) emoji process
Since many emojis in the text may represent the user's sentiment, we use the package emote [@emote] to process the emoji in the context. Moreover, this package could help us to transform the emoji into relative English. For example, the smiling face may be transformed into the "happy" word.

The cleaning work is implemented by using the spacy[@Honnibal_spaCy_Industrial-strength_Natural_2020] pipeline.

Since the processing time of this cleaning takes too much time (10 hours for 3000 records). The data would be processed and saved as Excel files and used to deploy later.

As introduced above, some data may not be in English or not spelled correctly. In this case, the return of the doc._.outcome_spellCheck would be null, which means some data contains the NA value in the data set. So we need to drop the NA value.

\newpage

### Google Play

![The wordclod per class of labels for train data]("./Word_train_filt.png")

![The wordclod per class of labels for test data]("./Word_test_filt.png")

By comparing the word cloud and shape of data before cleaning, we could find that, although the highest frequency keyword for the different labels would still be 'game,' the frequency is not higher than before, and other words like 'reward' get higher frequency.

\newpage

### Twitter

![The wordclod for Twitter data]("./Word_valid_filt.png")

Here is the word cloud for Twitter's data; according to the plot, we could find that some terms, like 'GenshinImpact,' 'Adventure,' and 'Rank,' and so on, have changed.

## Data Balance

![The Count Per Class of Labels for balanced train data]("./Class_label-train_bal.png"){width=70%}

\newpage

![The wordclod per class of labels for balanced train data]("./Word_train_bal.png")

There is the visualization of the training data we used in the later model training; we could find that all the label has been balanced, and all have the same count number of the different classes. However, the word cloud seems that not too much change.

# Method. 

First, the data has already been divided into training and test data. Then, the SMOTE mentioned previously would be used on train data to balance the data. 

After we have balanced training data, we could do the necessary data preprocessing based on different models' needs. For example, for a non-deep learning model, the preprocessing would be vectorization. Moreover, for the deep learning model, the preprocessing would be tokenization.

Then we could first apply the non-deep learning models, like LSVC and Logistic regression. Then applies the deep learning models. Finally, we would select the best performance model to predict Twitter's data after training all the models.

\newpage

# Results. 

## Google Play

![Result-Summary]("./Result-Summary.PNG")

We first focus on the Kappa score. According to the definition, we could find that the BERT model takes the highest score, which means it has the highest classification precision among those models. Then we come to the Fbeta score, in which the linear regression model has the highest score, but the BERT model has a close score to the linear regression model. In this report, we set the beta value to 1, which means there is a balanced evaluation between the focus of recall and the precision. Finally, we come to the AUC score; we could find that the LSTM and BERT have a similar score, much higher than the two other models. 

In this case, according to the result above, we could find that the BERT model has the best performance among these models. Therefore, we would use the BERT model to predict Twitter's data in this case.

## Twitter

```{r}
print(read_xlsx("valid_res.xlsx"))
```

\newpage

![The Count Per Class of Labels for Twitter data]("./Class_label-valid_res.png"){width=70%}

Here is the result of the prediction by using the BERT model. According to the result, we could find that the model predicts this data most belongs to class 3, which is the neutral one. 

# Discussion. 

We used the BERT model in the previous section to predict Twitter's data. We could find that the number of scores means more people give positive comments on this game than negatively. However, according to Google play's data, which is shown below. Most people comment highly on this game, and other scores seem to have similar numbers.

```{r}
ori = read_excel("data_38k_us.xlsx")
cur = read_xlsx("valid_res.xlsx")
tes = rbind(ori[ori$Time %like% c('2022-01-09'),], ori[ori$Time %like% c('2022-01-10'),])
```

```{r fig.width=10}
ggplot(tes)+geom_histogram(aes(x=Score),binwidth=0.5) + ggtitle('2022-01-09 & 2022-01-10')
```

In this case, there is maybe some problem with the prediction data (Twitter's data) or the original data (Google Play's data). So, we select some data from different scores in both data sets. Then, we would use the random sampling function built in R with the seed number of 2022.


```{r, width = 100}
set.seed(2022)
TNR = data.frame(matrix(nrow = 5, ncol = 3))

colnames(TNR) <- c('Score','Google','Twitter')

TNR[,1] = c(1:5)

TNR[1,2] = slice_sample(tes[tes$Score == "1",])$Content
TNR[2,2] = slice_sample(tes[tes$Score == "2",])$Content
TNR[3,2] = slice_sample(tes[tes$Score == "3",])$Content
TNR[4,2] = slice_sample(tes[tes$Score == "4",])$Content
TNR[5,2] = slice_sample(tes[tes$Score == "5",])$Content

TNR[1,3] = slice_sample(cur[cur$Score == "1",])$Tweet
TNR[2,3] = slice_sample(cur[cur$Score == "2",])$Tweet
TNR[3,3] = slice_sample(cur[cur$Score == "3",])$Tweet
TNR[4,3] = slice_sample(cur[cur$Score == "4",])$Tweet
TNR[5,3] = slice_sample(cur[cur$Score == "5",])$Tweet
```

\newpage

```{r}
print("Google Play:")
```

Score 1:"At least give us some more primogems. If even lvl 80 trial characters can't complete the current event with lvl 65 monsters, what are *we* supposed to do."

Score 2:"Bounty quest in liyue,the enemy was still within the bounty area but it refilled it's HP"

Score 3:"Best RPG I've ever played, great storyline and characters the reason I've only put 3 stars is due to the amount of space this game takes up. Within the next couple of updates I'll have to delete the app for good because it takes most of my storage"

Score 4:"Server: Europe Adventure Rank: 56 almost 57 First 5\*: Tartaglia Last 5\*: Red Main character: Albedo/Kazuha New weapon: Black Swords element: Anemo/Geo Fav region in Teyvat: Dragonspine Fan food: Matsutake Meat Rolls Blue colour: Curcuma  #GenshinImpact #!"

Score 5:"i love the game, but from time to time, I tend to forget about the game since sometimes it gets boring, but overall, wonderful game :)"


```{r}
print("Twitter:")
```

Score 1: "\@SleeplesslyS \@GenshinImpact This is disappointing. I didn't notice before but...person_facepalming . female_sign_?I feel huge disappointment towards them for doing this."

Score 2:"i miss text :( \@GenshinImpact RELEASE HIM FROM THE BASEMENT"

Score 3:"My Server:EU My Adventure Rank:45 My first 5white_medium_star️:Tartaglia My last 5white_medium_star️:Jean My favourite character:Sucrose My favourite weapon:polearm My favourite element:water My favourite region in Teyvat: Inazuma  My favourite food: Tontsuku Ramen My Seelie colour: orange  #GenshinImpact #!"

Score 4:"This game is awesome! This is the only RPG I play, because most of them aren't interesting enough. I've been playing This for months. 10 out of 10 would recommend."

Score 5:"Mission success!  We are a #shenhe haver!....by drawing her :)  I think my art improved  #GenshinImpact #."

According to the data above, we could find that there is no special problem for both data sets, which shows a reasonable reason for being graded correctly. However, it is for humans, but not for machines. So, for example, we could find that Score 3 in Google play's data contains the word 'best' but gets a grade of 3. In this case, it may 'mislead' the models like linear regression. But for, a model like the BERT, which considers the whole sentence but not just words, would avoid a problem like this.
    	

# Conclusion. 


By comparing the results above, we could find that all these models we used in this paper do not perform well on these data. However, the BERT is the best model among them. 

If we want to get a well-performance machine learning model, it is essential to have a high-quality and large enough data set for training and testing since it is the basis of a model. The model learns the feature(information) from this data set and applies it to other places. However, due to the limitation of the computer resources, we have to use a small data set, which may not contain enough features for models to learn. Besides, the quality of the data is also not good enough. Although it is common in real life that the information is imbalanced, the method we used here, oversampling, is not enough for balancing the data, which also brings a bias to the data. For further work, we may consider combining oversampling and undersampling to create more balanced data.

What is more, although the deep learning model usually performs better than the non-deep learning model when applied to some extensive data analysis, it takes time and needs to find the best parameters for it. Like in this paper, the performance of the four models is quite similar. However, the non-deep learning model results much quicker and easier than the deep learning model.

# Further work

In order to improve the performance of the model, we could have a try on the method below:

  - 1) Try to use the combination of oversampling and undersampling to create more balanced data.
  
  - 2) Try to use other measurements(metrics) for models.
  
  - 3) Try to use other kinds of models, for example, CNN, Etc.
  
  - 4) If possible, try to use a complete data set.

# Reference
